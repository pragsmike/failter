# **User Guide: Conducting Experiments with Failter**

This guide explains how to use the **Failter** command-line tool to test, evaluate, and optimize your prompts.

## 1. Purpose

Failter is an experimentation framework designed to help you answer questions like:

*   "Which of my prompt variations performs best for this task?"
*   "Which LLM model is the most effective and efficient for my prompt?"
*   "How consistent is a model's performance across different input documents?"

It automates the tedious process of running numerous tests, evaluating the results, and summarizing them so you can focus on making data-driven decisions to improve your prompts.

## 2. The Experiment Workflow

The Failter workflow consists of three main steps, each corresponding to a command you will run:

1.  **`experiment`**: You provide a set of inputs, prompt templates, and models. Failter runs every possible combination and generates the filtered output files.
2.  **`evaluate`**: Failter uses a powerful "judge" LLM (like GPT-4o) to automatically review and grade the quality of each output file from the previous step.
3.  **`report`**: Failter gathers all the metadata, performance metrics, and evaluation grades into a comprehensive summary report, delivered as both a markdown table and a CSV file.

## 3. Setting Up Your Experiment

First, create a single directory for your experiment. Inside this directory, you must create the following structure:

```
my-prompt-test/
├── inputs/
│   ├── article_to_clean.md
│   └── another_document.txt
├── templates/
│   ├── prompt_v1.md
│   └── prompt_v2_more_specific.md
└── model-names.txt
```

#### `inputs/` directory
This folder contains all the source files you want to test your prompts against.
*   The content can be plain text, markdown, or any other format the LLM can process.
*   Having multiple input files helps test the consistency and reliability of your prompts.

#### `templates/` directory
This folder contains your different prompt variations.
*   Each file is a complete prompt template.
*   **Crucially**, each template file **must** contain the placeholder token `{{INPUT_TEXT}}`. Failter will replace this token with the content of an input file during a trial.

**Example `prompt_v1.md`:**
```markdown
Please summarize the following article in three sentences.

---
{{INPUT_TEXT}}
```

#### `model-names.txt` file
This is a simple text file listing the LLM models you want to test.
*   List one model name per line.
*   The model names must match those configured in the system's LLM proxy (e.g., `ollama/qwen3:8b`, `openai/gpt-4o-mini`).

## 4. Running the Experiment

Open your terminal or command prompt and navigate to the Failter project's root directory. All commands are run from there.

#### Step 1: Execute the Experiment
This command will create a new `results/` directory inside your experiment folder and populate it with the filtered output from each trial.

```bash
# We strongly recommend a "dry run" first to verify your setup
clj -M:run experiment path/to/my-prompt-test --dry-run

# Once verified, run the live experiment
clj -M:run experiment path/to/my-prompt-test
```
*This step can take a long time, depending on the number of inputs, templates, models, and the speed of the models.*

#### Step 2: Evaluate the Results
This command will create `.eval` files containing a grade and rationale for each successful trial.

```bash
clj -M:run evaluate path/to/my-prompt-test
```

#### Step 3: Generate the Final Report
This is the final step. It will print a summary table to your screen and also save `report.md` and `report.csv` inside your experiment directory.

```bash
clj -M:run report path/to/my-prompt-test
```

## 5. Finding and Interpreting the Results

After running the full pipeline, your experiment directory will look like this:

```
my-prompt-test/
├── inputs/
├── templates/
├── results/
│   ├── openai-gpt-4o-mini_prompt-v1/
│   │   ├── article_to_clean.md         <-- The filtered output
│   │   ├── article_to_clean.md.eval    <-- The judge's evaluation
│   │   └── article_to_clean.md.thoughts  <-- The model's "internal monologue" (if any)
│   └── ollama-qwen3-8b_prompt-v2/
│       └── ...
├── model-names.txt
├── report.md                             <-- Your final summary (human-readable)
└── report.csv                            <-- Your final summary (for spreadsheets)
```

### Understanding the Output Files

*   **Filtered Output (`article_to_clean.md`)**: This is the main result. Open it to inspect the text generated by the model. Its YAML frontmatter contains valuable metadata:
    *   `filtered-by-model`: The model used.
    *   `filtered-by-template`: The prompt template used.
    *   `execution-time-ms`: How long the LLM call took.
    *   `token-usage`: The number of tokens processed.
    *   `estimated-cost`: The cost of the API call, if available.
    *   `error`: If the trial failed, this key will contain the error message.
*   **Thoughts File (`.thoughts`)**: If the model generated an "internal monologue" (e.g., inside `<think>...</think>` tags), it is saved here. This is extremely useful for debugging *why* a model is misinterpreting your prompt.
*   **Evaluation File (`.eval`)**: This contains the structured YAML output from the judge model, including a `grade` (A-F) and a `rationale`.

### Interpreting the Final Report

The `report.md` and `report.csv` files provide the high-level summary for decision-making.

| Model                 | Template         | Avg Score | Avg Time(s) | Avg Cost  | Trials | Errors | Grade Distribution |
| --------------------- | ---------------- | --------- | ----------- | --------- | ------ | ------ | ------------------ |
| openai/gpt-4o-mini    | prompt_v2        | 4.80      | 5.21        | $0.00015  | 5      | 0      | `{"A" 4, "B" 1}`   |
| ollama/qwen3:8b       | prompt_v2        | 4.20      | 35.80       | $0.00000  | 5      | 0      | `{"A" 2, "B" 2}`   |
| openai/gpt-4o-mini    | prompt_v1        | 3.00      | 4.95        | $0.00014  | 5      | 0      | `{"C" 5}`          |
| ollama/qwen3:8b       | prompt_v1        | 1.00      | 60.01       | $0.00000  | 5      | 5      | `{"F" 5}`          |

*   **Avg Score**: The primary metric for quality (A=5, B=4, etc.). Higher is better.
*   **Avg Time(s) / Avg Cost**: Metrics for performance and efficiency. Lower is better.
*   **Errors**: The number of trials that failed due to technical issues (like a timeout).
*   **Grade Distribution**: The most important column for understanding consistency. A combination with `{"A" 4, "F" 1}` is less reliable than one with `{"B" 5}`, even if their average scores are similar.

From the example report above, you can conclude that **`prompt_v2` is clearly superior to `prompt_v1`**, and `gpt-4o-mini` provides the highest quality results, while `qwen3:8b` is a strong, free alternative if you can tolerate slightly lower consistency and longer processing times.
