Based on the search results, here are the next largest models that will give higher success rates for text processing tasks:

## 7-8B Parameter Models (~95% Success Rate):

### **1. Qwen2.5-7B-Instruct** - **TOP CHOICE**
Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts. This is specifically optimized for instruction following tasks.

### **2. Llama 3.1 8B** - **STRONG SECOND**
Llama 3.1 8B is mentioned as one of the top small language models. Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.

### **3. LFM-7B (Liquid AI)** - **EMERGING OPTION**
The world's best-in-class English, Arabic, and Japanese model, native in French, German, and Spanish, optimized to be the substrate for private enterprise chat, code, fast instruction following, and agentic workflows.

### **4. Mistral 7B v0.3** - **RELIABLE FALLBACK**
The 7B model released by Mistral AI, updated to version 0.3. Proven track record for instruction following.

## 20-30B Parameter Models (~98% Success Rate):

### **1. Mistral Small 3 (24B)** - **PREMIUM CHOICE**
Released in January 2025, Mistral Small 3 Instruct is a 24-billion-parameter model that achieves state-of-the-art capabilities comparable to larger models. It is ideal for various text generation tasks, including fast-response conversational agents, low-latency function calling.

### **2. Qwen2.5-32B-Instruct** - **IF AVAILABLE**
Based on the Qwen family's strong instruction-following capabilities.

## Why These Excel at Text Processing:

The 7-8B models hit the **sweet spot** for structured tasks because they have:
- **Sufficient parameter count** for complex pattern recognition
- **Strong instruction-following** training specifically for Qwen2.5 and Llama 3.1
- **JSON/structured output** capabilities mentioned for Qwen2.5
- **Better consistency** than smaller models

## Expected Success Rates Summary:

| Model Size | Success Rate | Examples |
|------------|--------------|----------|
| 1-3B | 80-90% | Phi-3.5 Mini, Llama 3.2 1B |
| **7-8B** | **95%** | **Qwen2.5-7B, Llama 3.1 8B** |
| 20-30B | 98% | Mistral Small 3 (24B) |

## Recommendation:

**Qwen2.5-7B-Instruct** is your best bet - it's specifically designed for instruction following and structured data processing, which is exactly what your markdown cleanup task requires. The jump from 3B to 7B parameters provides significant reliability improvement for this type of rule-based text manipulation.
