# **Failter: Design Document**

**Version:** 3.2
**Audience:** Developers joining the project who need to understand its architecture, design principles, and how to contribute.

## 1. Overview & Philosophy

**Failter** is a command-line framework for systematically filtering text using Large Language Models (LLMs). Its primary purpose is to serve as an experimentation harness to compare the performance of different models and prompt engineering strategies for specific text transformation tasks.

The core philosophy is to treat **prompts as code**. Instead of implementing filtering logic in a traditional programming language, Failter defines transformations in natural language via prompt templates. This allows for rapid iteration on the logic itself. The framework then provides the necessary machinery to rigorously test, evaluate, and analyze the performance of these prompts across a wide range of variables.

A key extension of this philosophy is that prompt templates themselves can be self-describing artifacts, optionally containing their own YAML frontmatter for metadata, which is ignored by the Failter runtime.

## 2. Core Concepts

The system is built around two first-class data structures, defined as records:

*   **`Trial` Record:** The atomic unit of work. A single trial represents the plan to process one **input file** with one **prompt template** using one **LLM model**. After execution, this record is hydrated with performance metrics (`execution-time-ms`, `cost`) or an `error`.

*   **`Eval` Record:** The result of a qualitative assessment. It contains the `Trial` record it is assessing, along with a normalized numeric `score` (0-100), a `rationale` from the judge LLM, the `method` of evaluation used, and the `judge-model` that performed the assessment. The interpretation and display of the `score` is handled by the active scoring strategy.

*   **Experiment:** A collection of trials defined by a self-contained directory. An experiment holds all inputs, templates, models, and the on-disk artifacts generated by trial runs and evaluations.

## 3. System Architecture & Data Flow

Failter operates as a three-stage pipeline, orchestrated by the user via command-line subcommands. The key design principle is the flow of explicit data records (`Trial`, `Eval`) between these stages.

```
+------------+      +--------------------+      +----------------+      +-------------------+
| <exp_dir>/ |----->| failter experiment |----->|  Seq of Trial  |----->|  failter runner   |
+------------+      | (Orchestrator)     |      |    records     |      |    (Executor)     |
                    +--------------------+      +----------------+      +-------------------+
                                                                                |
                                                                                V
                                                                    +----------------------+
                                                                    | <exp_dir>/results/   |
                                                                    |  (Hydrated files)    |
                                                                    +----------------------+
                                                                                |
                                                                                |
                                        +---------------------+                 |
                                        | failter.scoring     |<----------------+
                                        | (Scoring Strategy)  |                 |
                                        +---------------------+                 |
                                                 ^   ^                          |
+------------+      +------------------+         |   |         +----------------+
| <exp-dir>/ |----->| failter evaluate |---------+   |-------->|  Seq of Eval   |
+------------+      |    (Producer)    |             |         |    records     |
                    +------------------+             |         +----------------+
                                                     |                |
                                                     |                V
+------------+      +-----------------+              +       +-----------------+
| <exp_dir>/ |----->| failter report  |--------------+------>| report.md,      |
+------------+      |   (Consumer)    |                      | report.csv      |
                    +-----------------+                      +-----------------+
```

The `evaluate` and `report` stages are decoupled from the specific logic of how a score is requested, parsed, and displayed. They delegate this "policy" to the `failter.scoring` namespace.

## 4. Key Design Principles

*   **Filesystem as Database:** The entire state of an experiment—its inputs, configuration, results, and evaluations—is contained within its directory. This makes experiments highly portable, easily browsable, and naturally compatible with version control.

*   **First-Class Data Records:** The system avoids passing around ad-hoc maps. The `Trial` and `Eval` records provide a clear, self-documenting structure for the data that flows between the major components of the application.

*   **Pluggable Strategies via Multimethods:** The "policy" for how to evaluate and score a trial is decoupled from the "mechanism" of running the evaluation. By using Clojure's multimethods in the `failter.scoring` namespace, we can easily add new scoring strategies (e.g., letter grades, numeric scales, star ratings) without changing the core `evaluator` or `reporter` logic.

*   **Centralized Configuration:** All application configuration (API endpoints, default models, timeouts, active scoring strategy, etc.) is stored in the `failter.config` namespace, providing a single source of truth.

## 5. Ground Truth Evaluation

The framework includes a sophisticated, optional evaluation method that compares a model's output against a human-verified "perfect" example. This feature is fully compatible with the pluggable scoring system. The active scoring strategy will be applied whether a `ground-truth` or `rules-based` evaluation is performed.

## 6. Codebase Tour (Module Breakdown)

*   `failter.core`: Main application entry point and command-line dispatcher.
*   `failter.config`: A single map containing all application configuration, including the active `:scoring-strategy`.
*   `failter.util`: A utility belt for small, generic, pure helper functions.
*   `failter.exp-paths`: The definitive API for the experiment's directory structure. **Finds all result files**, excluding Failter's own `.eval` and `.thoughts` artifacts.

*   `failter.trial`: Defines the `Trial` record and its constructors.
*   `failter.eval`: Defines the `Eval` record (which contains a numeric `:score`) and its I/O helpers.

*   **`failter.scoring`**: The central hub for all scoring-related logic. It uses multimethods to define a "scoring strategy" protocol, which consists of three functions: `get-prompt-instructions`, `parse-raw-score`, and `format-score-distribution`. It provides concrete implementations for different strategies (e.g., `:letter-grade`, `:numeric-100`).

*   `failter.experiment`: **Produces** a sequence of `Trial` records.
*   `failter.runner`: **Consumes** a `Trial` record and executes it. It is responsible for **parsing prompt template files** to extract the body of the prompt, ignoring any YAML frontmatter.
*   `failter.evaluator`: **Consumes** completed `Trial` records and **produces** `Eval` records. It is decoupled from scoring logic. It calls `failter.scoring` to get prompt instructions and parse the numeric score. It also **parses prompt template files** to provide the correct prompt body to the judge LLM.
*   `failter.reporter`: A pure **consumer** of `Eval` records. It is also decoupled from scoring logic; it calls `failter.scoring` to get a formatted string representing the distribution of scores for the final report.

*   `failter.llm-interface` / `failter.frontmatter`: Lower-level utility namespaces.

## 7. How to Extend the System

This architecture makes the system easy to extend in a modular way.

### To Add a New Scoring Strategy

The scoring system is designed to be easily extensible. To add a new method (e.g., a 1-5 star rating), follow these three steps:

**1. Choose a Keyword for Your Strategy:**
   Pick a simple keyword, for example `:star-rating`.

**2. Add Implementations for the Strategy's Three Behaviors:**
   Open `src/failter/scoring.clj` and add a new set of `defmethod` implementations for your keyword. You must implement all three.

   ```clojure
   ;; In src/failter/scoring.clj

   ;; --- Strategy 3: :star-rating (Example) ---

   (defmethod get-prompt-instructions :star-rating [_]
     "## Scoring Scale:\n
   Rate the output on a scale of 1 to 5 stars...
   ## Output Format:
   ```yaml
   stars: [An integer from 1 to 5]
   rationale: [Your rationale...]
   ```")

   (defmethod parse-raw-score :star-rating [_ response-text]
     ;; Your logic to parse the 'stars' field from the LLM response.
     ;; IMPORTANT: Must return a normalized score between 0 and 100.
     ;; For example, 1 star = 20, 2 stars = 40, etc.
     (let [stars (Integer/parseInt (second (re-find #"stars:\s*([1-5])" response-text)))]
       (* stars 20)))

   (defmethod format-score-distribution :star-rating [_ scores]
     ;; Your logic to create a summary string for the report.
     (let [score-to-stars #(str (quot % 20) "⭐")
           dist (frequencies (map score-to-stars scores))]
       (pr-str (into (sorted-map) dist))))
   ```

**3. Activate Your Strategy:**
   Open `src/failter/config.clj` and change the `:scoring-strategy` value to your new keyword.

   ```clojure
   ;; In src/failter/config.clj
   ...
   :evaluator {:scoring-strategy :star-rating ;; <-- Activate your new strategy
   ...
   ```
   No other files need to be changed. The `evaluator` and `reporter` will automatically use your new logic.

### To Add a New Reporter Format (e.g., JSON)

1.  In `reporter.clj`, create a new function `(format-as-json [summaries])`.
2.  In the `generate-report` function, call your new function and `spit` the result to `report.json`. No other modules are affected.
