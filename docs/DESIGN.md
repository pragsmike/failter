# **Failter: Design Document**

**Version:** 2.0
**Audience:** Developers joining the project who need to understand its architecture, design principles, and how to contribute.

## 1. Overview & Philosophy

**Failter** is a command-line framework for systematically filtering text using Large Language Models (LLMs). Its primary purpose is to serve as an experimentation harness to compare the performance of different models and prompt engineering strategies for specific text transformation tasks.

The core philosophy is to treat **prompts as code**. Instead of implementing filtering logic in a traditional programming language, Failter defines transformations in natural language via prompt templates. This allows for rapid iteration on the logic itself. The framework then provides the necessary machinery to rigorously test, evaluate, and analyze the performance of these prompts across a wide range of variables.

## 2. Core Concepts

*   **Trial:** The atomic unit of work. A single trial consists of processing one **input file** with one **prompt template** using one **LLM model** to produce one **output file**.
*   **Experiment:** A collection of trials defined by a self-contained directory. An experiment holds all inputs, templates, models, and generated results.
*   **Result Artifacts:** The files generated by a trial run. This includes:
    *   **Primary Output (`.md`):** The filtered text file. Its YAML frontmatter is a rich record of the trial.
    *   **Monologue File (`.thoughts`):** An optional file containing the model's "internal monologue" (e.g., text from within `<think>` tags), used for debugging the model's reasoning process.
    *   **Evaluation File (`.eval`):** A file containing a qualitative assessment (`grade` and `rationale`) from a "judge" LLM.
*   **Frontmatter-Aware Processing:** The framework explicitly handles YAML frontmatter. It parses metadata from input files, passes only the body text to the LLM, and then writes an amended set of metadata back to the output file. This simplifies the LLM's task and enriches the output data.

## 3. System Architecture & Workflow

Failter operates as a three-stage pipeline, orchestrated by the user via command-line subcommands.

```
+----------------+      +-------------------------+      +-----------------------------+
|      USER      |----->|   failter experiment    |----->|  <exp_dir>/results/         |
+----------------+      |   (Orchestrator)        |      |  ├── trial-1/               |
                        +-------------------------+      |  │   ├── output.md          |
                                                         |  │   └── output.md.thoughts |
                                                         |  └── trial-2/               |
                                                         |      └── ...                |
                                                         +-----------------------------+
                                                                       |
+----------------+      +-------------------------+                    |
|      USER      |----->|    failter evaluate     |<-------------------+
+----------------+      |      (Judge)          |
                        +-------------------------+
                                     |
                                     V
                        +-----------------------------+
                        |  <exp_dir>/results/         |
                        |  ├── trial-1/               |
                        |  │   ├── output.md          |
                        |  │   ├── output.md.eval     |  <-- .eval files are added
                        |  │   └── ...                |
                        |  ...                        |
                        +-----------------------------+
                                     |
+----------------+      +-------------------------+                    |
|      USER      |----->|     failter report      |<-------------------+
+----------------+      |      (Analyzer)         |
                        +-------------------------+
                                     |
                                     V
                        +-----------------------------+
                        |  <exp_dir>/                 |
                        |  ├── report.md              |
                        |  └── report.csv             |
                        +-----------------------------+
```

## 4. Filesystem as Database

A key design decision is to use the filesystem as the sole data store for experiments.

*   **State:** The entire state of an experiment—its inputs, configuration, results, and evaluations—is contained within its directory.
*   **Rationale:** This approach avoids the complexity of a database setup. It makes experiments highly portable, easily browsable, and naturally compatible with version control systems like Git.
*   **Data Integrity:** The primary output files (`.md`) are the source of truth for each trial. Their YAML frontmatter contains a complete, immutable record of the trial's parameters and results (time, cost, errors). The `reporter` relies on this metadata, not directory names, for its analysis.

## 5. Codebase Tour (Module Breakdown)

The codebase is organized into distinct namespaces with clear responsibilities.

*   `src/failter/core.clj`
    *   **Responsibility:** Main application entry point and command-line dispatcher.
    *   **Details:** Parses subcommands (`experiment`, `evaluate`, `report`) and their arguments, then delegates to the appropriate namespace. It is the "switchboard" of the application.

*   `src/failter/experiment.clj`
    *   **Responsibility:** Orchestrates an experiment by generating trial parameters.
    *   **Details:** `conduct-experiment` reads the `inputs`, `templates`, and `model-names.txt` files. It creates a list of all possible trial combinations and constructs the output paths inside a `results/` directory. It is decoupled from execution and simply calls a provided `trial-fn` for each set of parameters.

*   `src/failter/runner.clj`
    *   **Responsibility:** Executes a single, well-defined trial.
    *   **Details:** `run-single-trial` is the workhorse. It uses `frontmatter.clj` to parse the input file, sends *only the body* to the LLM, measures execution time, and handles both success and error cases. On success, it writes the filtered output, amended frontmatter, and an optional `.thoughts` file. On failure, it writes a file containing only the frontmatter with an `:error` key.

*   `src/failter/evaluator.clj`
    *   **Responsibility:** Manages the qualitative evaluation of trial results.
    *   **Details:** `run-evaluation` scans an experiment's `results/` directory for successful `.md` files that lack a corresponding `.eval` file. It constructs a judge prompt and calls the judge model, saving the resulting evaluation.

*   `src/failter/reporter.clj`
    *   **Responsibility:** Analyzes all trial results and generates summary reports.
    *   **Details:** `generate-report` is the entry point. It scans the `results/` directory for all primary `.md` files (not `.thoughts` or other files). It reads metadata directly from the frontmatter. For trials that failed (i.e., have an `:error` key), it synthetically assigns a grade of "F". It then aggregates the data by model and template, calculates statistics, and formats the output as both a markdown table and a CSV file.

*   `src/failter/llm-interface.clj`
    *   **Responsibility:** Low-level communication with the LLM API proxy (LiteLLM).
    *   **Details:** `call-model` handles the HTTP POST request. It returns a rich map containing the LLM's response (`:content`, `:usage`, `:cost`) on success, or an `:error` map on failure (e.g., timeout, API error).

*   `src/failter/frontmatter.clj`
    *   **Responsibility:** A stateless utility for handling YAML frontmatter.
    *   **Details:** Provides `parse-file-content` and `serialize` functions, centralizing the logic for interacting with files that contain frontmatter.

## 6. How to Extend the System

Failter is designed to be extensible. Common scenarios include:

#### A. Adding a New Filtering Task
This requires **no code changes**.
1.  Create a new prompt file in a `templates/` directory.
2.  Set up the rest of the experiment directory (`inputs/`, `model-names.txt`).
3.  Run the `experiment`, `evaluate`, and `report` commands.

#### B. Adding a New Reporter Format (e.g., JSON)
1.  **In `reporter.clj`:**
    *   Create a new function `(format-as-json [summaries])`.
    *   In `generate-report`, call this new function and add a `spit` command to write `report.json`.
2.  **No other modules are affected.**

#### C. Supporting a Non-OpenAI-Compatible LLM API
This change is isolated to `llm-interface.clj`.
1.  Modify `call-model` to construct the request in the new format.
2.  Modify `parse-llm-response` to extract `:content`, `:usage`, and `:cost` from the new response structure. The rest of the system will continue to function as long as `call-model` returns a map with these expected keys (or an `:error` key).
