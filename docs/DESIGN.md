# **Failter: Design Document**

**Version:** 1.0
**Audience:** Developers joining the project who need to understand its architecture, design principles, and how to contribute.

## 1. Overview

**Failter** is a command-line framework for systematically filtering text using Large Language Models (LLMs). Its primary purpose is to serve as an experimentation harness to compare the performance of different models and prompt engineering strategies for specific text transformation tasks.

Instead of hard-coding filtering logic (e.g., complex regex), Failter defines transformations in natural language via **prompt templates**. It then automates the process of running these transformations across various models and input files, evaluating the results, and generating comparative reports.

## 2. Core Concepts

Understanding Failter requires familiarity with these key concepts:

*   **Trial:** The atomic unit of work in Failter. A single trial consists of processing one **input file** with one **prompt template** using one **LLM model** to produce one **output file**.
*   **Experiment:** A collection of trials defined by a directory structure. An experiment directory contains all the necessary components: a set of input files, a set of prompt templates, and a list of model names.
*   **Result Artifacts:** The files generated by a trial run. This includes the primary filtered output file (e.g., `a.md`), an optional file containing the model's internal monologue (`a.md.thoughts`), and a file containing a qualitative assessment (`a.md.eval`).
*   **Evaluation:** The process of using a powerful "judge" LLM to assess the quality of a trial's output. The judge model is given the original text, the prompt, and the filtered output, and it produces a structured grade and rationale.
*   **Report:** A summary table generated from all evaluation results within an experiment, providing a high-level comparison of the performance of each `model-template` combination.

## 3. System Architecture & Workflow

Failter is designed as a modular command-line tool with a pipeline-oriented workflow. The user interacts with the system through three main subcommands:

1.  **`failter experiment <dir>`**: This is the starting point.
    *   The `experiment` command reads the `inputs/`, `templates/`, and `model-names.txt` files from the specified experiment directory `<dir>`.
    *   It orchestrates the execution of all possible trial combinations.
    *   For each trial, it creates a unique output directory under `<dir>/results/` (e.g., `results/qwen3-8b_small/`).
    *   It then calls the **runner** to process the trial, which involves sending the input text and prompt template to the specified LLM.
    *   The runner creates the final output file (e.g., `a.md`) inside the trial directory, prepending it with rich YAML frontmatter that includes execution time, token usage, cost, and any errors.

2.  **`failter evaluate <dir>`**: This command assesses the quality of the generated results.
    *   The `evaluate` command scans the `<dir>/results/` directory for any output files (`.md`) that do not yet have a corresponding `.eval` file.
    *   For each unevaluated (and successful) trial, it constructs a new prompt for a "judge" model using the `evaluation-prompt.md` template.
    *   This prompt includes the *body* of the original input, the prompt template used, and the *body* of the generated output.
    *   The judge model's response (a YAML block with a `grade` and `rationale`) is saved to the `.eval` file.

3.  **`failter report <dir>`**: This command synthesizes the results into a human-readable summary.
    *   The `report` command scans the `<dir>/results/` directory, finding all primary output files (`.md`).
    *   For each file, it reads the metadata from the frontmatter and, if it exists, the corresponding `.eval` file.
    *   If a trial failed (indicated by an `:error` key in the frontmatter), it is automatically assigned a grade of "F".
    *   It then aggregates all this data, calculates average scores, times, and costs for each `model-template` combination, and prints a summary table to the console.

## 4. Codebase Tour (Module Breakdown)

The codebase is organized into distinct namespaces, each with a single responsibility.

*   `src/failter/core.clj`
    *   **Responsibility:** Main application entry point and command-line dispatcher.
    *   **Key Functions:** `(-main [& args])` parses the command-line arguments and dispatches to the appropriate function in other namespaces (`experiment`, `evaluator`, `reporter`). It is the "switchboard" of the application.

*   `src/failter/experiment.clj`
    *   **Responsibility:** Orchestrates an experiment.
    *   **Key Functions:** `(conduct-experiment [dir trial-fn])` is the core function. It discovers all input files, templates, and models and generates the full list of trials. It constructs the parameters for each trial and calls the provided `trial-fn` (the "runner"). It does **not** know how to execute a trial itself.

*   `src/failter/runner.clj`
    *   **Responsibility:** Executes a single, well-defined trial.
    *   **Key Functions:** `(run-single-trial [params])` takes a map of parameters (model, input path, etc.), handles the frontmatter parsing, calls the LLM, measures execution time, handles errors, and writes the final output file(s) (`.md` and `.thoughts`).

*   `src/failter/evaluator.clj`
    *   **Responsibility:** Manages the evaluation of trial results.
    *   **Key Functions:** `(run-evaluation [dir])` scans the results directory for unevaluated files, constructs the judge prompt, calls the judge model, and writes the `.eval` file.

*   `src/failter/reporter.clj`
    *   **Responsibility:** Analyzes and summarizes all trial results.
    *   **Key Functions:** `(generate-report [dir])` is the entry point. It finds all trial outputs, reads their metadata and evaluations, handles failed trials gracefully, aggregates the data, and prints the final formatted report.

*   `src/failter/llm-interface.clj`
    *   **Responsibility:** Low-level communication with the LLM API proxy (LiteLLM).
    *   **Key Functions:** `(call-model [model prompt])` takes a model name and a prompt string, makes the HTTP POST request, and returns a rich map containing the LLM's response (`:content`, `:usage`, `:cost`) or an `:error` map on failure.

*   `src/failter/frontmatter.clj`
    *   **Responsibility:** A utility for handling YAML frontmatter.
    *   **Key Functions:** `(parse-file-content [str])` splits a string into a metadata map and a body. `(serialize [map str])` combines a metadata map and a body back into a single string.

## 5. Design Philosophy & Rationale

The architecture of Failter is guided by several key principles:

1.  **Filesystem as Database:** The entire state of an experiment (inputs, templates, results, evaluations) is stored on the filesystem. This makes the system simple, portable, and easily version-controlled with Git. There is no need for a separate database.
2.  **Decoupled Orchestration and Execution:** The `experiment` namespace knows *what* trials to run, but not *how* to run them. The `runner` knows *how* to run a single trial but has no knowledge of the overall experiment. This separation allows for powerful features like `--dry-run` by simply passing a different function (`print-trial-details`) to the orchestrator.
3.  **Idempotency and Resilience:** The framework is designed for long-running, potentially failing tasks. `experiment` skips already-completed trials, and `evaluate` skips already-evaluated ones. This means you can re-run commands multiple times, and the system will simply pick up where it left off. Individual trial failures are caught and logged without halting the entire batch.
4.  **Data-Rich, Self-Describing Artifacts:** The output files are not just raw text; their YAML frontmatter contains a complete record of how they were created (model, template, time, cost, errors). This makes the `results/` directory a self-contained, analyzable dataset, simplifying the reporting and evaluation stages significantly.

## 6. How to Extend the System

Failter is designed to be extensible. Here are common extension scenarios:

#### A. Adding a New Filtering Task

This is the most common use case and requires **no code changes**.
1.  Create a new prompt file (e.g., `prompts/summarization-prompt.md`) that describes the new task.
2.  Create a new experiment directory.
3.  Place your new prompt file in the `templates/` subdirectory.
4.  Run the `experiment`, `evaluate`, and `report` commands as usual.

#### B. Adding a New Reporter Format (e.g., CSV)

This requires code changes but demonstrates the modular design.
1.  **Add a flag:** In `core.clj`, modify the `report` command parsing to accept a `--format csv` flag.
2.  **Implement the new format:** In `reporter.clj`, create a new function `(print-as-csv [summaries])` that formats the summary data as a CSV string.
3.  **Wire it up:** In `generate-report`, add a `cond` or `case` statement to call the appropriate printing function based on the format flag passed from `core.clj`.

#### C. Using a Different LLM Backend

All direct API communication is isolated in `llm-interface.clj`.
1.  If the new backend is OpenAI-compatible, you may only need to change the `LITELLM_ENDPOINT` constant and ensure your LiteLLM proxy is configured for the new models.
2.  If the API response format is different, you will only need to modify `parse-llm-response` to correctly extract the `:content`, `:usage`, and `:cost` fields from the new JSON structure. The rest of the system will function without changes.
